{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23902,"sourceType":"datasetVersion","datasetId":18276},{"sourceId":412975,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":335621,"modelId":356636}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install xmltodict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:04:59.252592Z","iopub.execute_input":"2025-05-27T09:04:59.253071Z","iopub.status.idle":"2025-05-27T09:05:03.519086Z","shell.execute_reply.started":"2025-05-27T09:04:59.253052Z","shell.execute_reply":"2025-05-27T09:05:03.518418Z"}},"outputs":[{"name":"stdout","text":"Collecting xmltodict\n  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\nDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\nInstalling collected packages: xmltodict\nSuccessfully installed xmltodict-0.14.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport xmltodict\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.ops import nms, box_iou\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nimport seaborn as sns\nfrom collections import defaultdict\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:07.259914Z","iopub.execute_input":"2025-05-27T09:05:07.260634Z","iopub.status.idle":"2025-05-27T09:05:18.003042Z","shell.execute_reply.started":"2025-05-27T09:05:07.260592Z","shell.execute_reply":"2025-05-27T09:05:18.002509Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# VOC Classes\nVOC_CLASSES = [\n    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\nclass VOCDataset(Dataset):\n    def __init__(self, root_dir, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        self.image_paths = sorted(glob.glob(os.path.join(root_dir, \"JPEGImages\", \"*.jpg\")))\n        self.annotation_paths = sorted(glob.glob(os.path.join(root_dir, \"Annotations\", \"*.xml\")))\n\n        self.class_names = VOC_CLASSES\n        self.class_dict = {k: v for v, k in enumerate(self.class_names)}\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        anno_path = self.annotation_paths[idx]\n\n        img = Image.open(img_path).convert(\"RGB\")\n        img = np.array(img)\n\n        with open(anno_path) as f:\n            anno = xmltodict.parse(f.read())[\"annotation\"]\n\n        boxes = []\n        labels = []\n\n        objects = anno.get(\"object\", [])\n        if not isinstance(objects, list):\n            objects = [objects]\n\n        for obj in objects:\n            label = self.class_dict[obj[\"name\"]]\n            bbox = obj[\"bndbox\"]\n            box = [\n                float(bbox[\"xmin\"]),\n                float(bbox[\"ymin\"]),\n                float(bbox[\"xmax\"]),\n                float(bbox[\"ymax\"])\n            ]\n            boxes.append(box)\n            labels.append(label)\n\n        if self.transforms:\n            transformed = self.transforms(image=img, bboxes=boxes, class_labels=labels)\n            img = transformed[\"image\"]\n            boxes = transformed[\"bboxes\"]\n            labels = transformed[\"class_labels\"]\n\n        target = {\"boxes\": torch.tensor(boxes, dtype=torch.float32),\n                  \"labels\": torch.tensor(labels, dtype=torch.int64),\n                  \"image_path\": img_path}\n\n        return img, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:20.671965Z","iopub.execute_input":"2025-05-27T09:05:20.672791Z","iopub.status.idle":"2025-05-27T09:05:20.681022Z","shell.execute_reply.started":"2025-05-27T09:05:20.672764Z","shell.execute_reply":"2025-05-27T09:05:20.680278Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n\n\nclass YOLOResNet(nn.Module):\n    def __init__(self, num_classes=20, S=7, B=2):\n        super(YOLOResNet, self).__init__()\n        self.S = S\n        self.B = B\n        self.C = num_classes\n\n        resnet = models.resnet34(pretrained=False)\n        self.backbone = nn.Sequential(\n            *list(resnet.children())[:-2],\n            nn.AdaptiveAvgPool2d((S, S))\n        )\n        self.head = nn.Sequential(\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(1024, self.C + self.B * 5, kernel_size=1)\n        )\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x.permute(0, 2, 3, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:24.160721Z","iopub.execute_input":"2025-05-27T09:05:24.161197Z","iopub.status.idle":"2025-05-27T09:05:24.166387Z","shell.execute_reply.started":"2025-05-27T09:05:24.161172Z","shell.execute_reply":"2025-05-27T09:05:24.165815Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_model(model_path, device):\n    \"\"\"Load the trained model\"\"\"\n    model = YOLOResNet(S=7)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:27.326411Z","iopub.execute_input":"2025-05-27T09:05:27.326944Z","iopub.status.idle":"2025-05-27T09:05:27.330814Z","shell.execute_reply.started":"2025-05-27T09:05:27.326922Z","shell.execute_reply":"2025-05-27T09:05:27.330197Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def decode_yolo_output(output, conf_threshold=0.5, nms_threshold=0.4, img_size=448):\n    \"\"\"Decode YOLO output to bounding boxes\"\"\"\n    batch_size = output.shape[0]\n    all_boxes = []\n    \n    for b in range(batch_size):\n        boxes = []\n        scores = []\n        labels = []\n        \n        pred = output[b]  # [7, 7, 30]\n        \n        for row in range(7):\n            for col in range(7):\n                cell = pred[row, col]\n                \n                # Get class probabilities\n                class_probs = cell[10:]\n                class_id = torch.argmax(class_probs).item()\n                class_score = class_probs[class_id].item()\n                \n                # Check both bounding boxes in the cell\n                for box_idx in range(2):\n                    x, y, w, h, conf = cell[box_idx*5:(box_idx+1)*5]\n                    \n                    # Calculate final confidence\n                    final_conf = conf.item() * class_score\n                    \n                    if final_conf > conf_threshold:\n                        # Convert to absolute coordinates\n                        center_x = (col + x.item()) / 7\n                        center_y = (row + y.item()) / 7\n                        width = w.item()\n                        height = h.item()\n                        \n                        # Convert to corner coordinates\n                        x1 = (center_x - width / 2) * img_size\n                        y1 = (center_y - height / 2) * img_size\n                        x2 = (center_x + width / 2) * img_size\n                        y2 = (center_y + height / 2) * img_size\n                        \n                        # Clamp to image boundaries\n                        x1 = max(0, min(x1, img_size))\n                        y1 = max(0, min(y1, img_size))\n                        x2 = max(0, min(x2, img_size))\n                        y2 = max(0, min(y2, img_size))\n                        \n                        if x2 > x1 and y2 > y1:  # Valid box\n                            boxes.append([x1, y1, x2, y2])\n                            scores.append(final_conf)\n                            labels.append(class_id)\n        \n        if len(boxes) > 0:\n            boxes = torch.tensor(boxes)\n            scores = torch.tensor(scores)\n            labels = torch.tensor(labels)\n            \n            # Apply NMS\n            keep = nms(boxes, scores, nms_threshold)\n            boxes = boxes[keep]\n            scores = scores[keep]\n            labels = labels[keep]\n        else:\n            boxes = torch.empty((0, 4))\n            scores = torch.empty(0)\n            labels = torch.empty(0, dtype=torch.long)\n        \n        all_boxes.append({\n            'boxes': boxes,\n            'scores': scores,\n            'labels': labels\n        })\n    \n    return all_boxes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:30.440477Z","iopub.execute_input":"2025-05-27T09:05:30.441221Z","iopub.status.idle":"2025-05-27T09:05:30.450010Z","shell.execute_reply.started":"2025-05-27T09:05:30.441198Z","shell.execute_reply":"2025-05-27T09:05:30.449257Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    \"\"\"Calculate IoU between two boxes\"\"\"\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    if x2 <= x1 or y2 <= y1:\n        return 0.0\n    \n    intersection = (x2 - x1) * (y2 - y1)\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union = area1 + area2 - intersection\n    \n    return intersection / union if union > 0 else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:35.492637Z","iopub.execute_input":"2025-05-27T09:05:35.492925Z","iopub.status.idle":"2025-05-27T09:05:35.498086Z","shell.execute_reply.started":"2025-05-27T09:05:35.492902Z","shell.execute_reply":"2025-05-27T09:05:35.497313Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate_detection_metrics(model, dataloader, device, iou_thresholds=[0.5, 0.75], conf_threshold=0.5):\n    \"\"\"Comprehensive evaluation of object detection metrics\"\"\"\n    model.eval()\n    \n    # Storage for all predictions and ground truths\n    all_predictions = []\n    all_ground_truths = []\n    \n    # Per-class metrics storage\n    class_metrics = defaultdict(lambda: {\n        'tp': defaultdict(int), 'fp': defaultdict(int), 'fn': defaultdict(int),\n        'predictions': [], 'ground_truths': []\n    })\n    \n    print(\"ðŸ”„ Running evaluation...\")\n    \n    with torch.no_grad():\n        for batch_idx, (imgs, targets) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n            imgs = torch.stack(imgs).to(device)\n            outputs = model(imgs)\n            \n            # Decode predictions\n            predictions = decode_yolo_output(outputs.cpu(), conf_threshold=conf_threshold)\n            \n            for i in range(len(predictions)):\n                pred = predictions[i]\n                target = targets[i]\n                \n                # Store for overall metrics\n                all_predictions.append(pred)\n                all_ground_truths.append(target)\n                \n                # Store per-class data\n                for label in target['labels']:\n                    class_id = label.item()\n                    class_metrics[class_id]['ground_truths'].extend(target['boxes'][target['labels'] == label])\n                \n                for j, label in enumerate(pred['labels']):\n                    class_id = label.item()\n                    class_metrics[class_id]['predictions'].append({\n                        'box': pred['boxes'][j],\n                        'score': pred['scores'][j].item()\n                    })\n    \n    # Calculate metrics for different IoU thresholds\n    results = {}\n    \n    for iou_thresh in iou_thresholds:\n        print(f\"\\nðŸ“Š Calculating metrics for IoU threshold: {iou_thresh}\")\n        \n        # Overall metrics\n        total_tp, total_fp, total_fn = 0, 0, 0\n        all_precisions, all_recalls = [], []\n        class_aps = []\n        \n        # Per-class evaluation\n        class_results = {}\n        \n        for class_id in range(len(VOC_CLASSES)):\n            class_name = VOC_CLASSES[class_id]\n            \n            # Get predictions and ground truths for this class\n            pred_data = []\n            gt_boxes = []\n            \n            # Collect all predictions for this class across all images\n            for img_idx, pred in enumerate(all_predictions):\n                mask = pred['labels'] == class_id\n                if mask.sum() > 0:\n                    for j in range(mask.sum()):\n                        pred_data.append({\n                            'image_id': img_idx,\n                            'box': pred['boxes'][mask][j],\n                            'score': pred['scores'][mask][j].item()\n                        })\n            \n            # Collect all ground truths for this class\n            for img_idx, target in enumerate(all_ground_truths):\n                mask = target['labels'] == class_id\n                if mask.sum() > 0:\n                    for box in target['boxes'][mask]:\n                        gt_boxes.append({\n                            'image_id': img_idx,\n                            'box': box,\n                            'used': False\n                        })\n            \n            # Sort predictions by confidence\n            pred_data.sort(key=lambda x: x['score'], reverse=True)\n            \n            # Calculate TP, FP for this class\n            tp, fp = 0, 0\n            \n            for pred in pred_data:\n                best_iou = 0\n                best_gt_idx = -1\n                \n                # Find best matching ground truth\n                for gt_idx, gt in enumerate(gt_boxes):\n                    if gt['image_id'] == pred['image_id'] and not gt['used']:\n                        iou = calculate_iou(pred['box'], gt['box'])\n                        if iou > best_iou:\n                            best_iou = iou\n                            best_gt_idx = gt_idx\n                \n                # Check if it's a true positive\n                if best_iou >= iou_thresh:\n                    tp += 1\n                    gt_boxes[best_gt_idx]['used'] = True\n                else:\n                    fp += 1\n            \n            fn = len(gt_boxes) - tp\n            \n            # Calculate precision, recall, F1\n            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n            \n            # Calculate AP using precision-recall curve\n            if len(pred_data) > 0 and len(gt_boxes) > 0:\n                # Create binary labels for AP calculation\n                y_true = []\n                y_scores = []\n                \n                for pred in pred_data:\n                    best_iou = 0\n                    for gt in gt_boxes:\n                        if gt['image_id'] == pred['image_id']:\n                            iou = calculate_iou(pred['box'], gt['box'])\n                            best_iou = max(best_iou, iou)\n                    \n                    y_true.append(1 if best_iou >= iou_thresh else 0)\n                    y_scores.append(pred['score'])\n                \n                if len(set(y_true)) > 1:  # Need both positive and negative samples\n                    ap = average_precision_score(y_true, y_scores)\n                else:\n                    ap = 0.0\n            else:\n                ap = 0.0\n            \n            class_results[class_name] = {\n                'tp': tp, 'fp': fp, 'fn': fn,\n                'precision': precision, 'recall': recall, 'f1': f1, 'ap': ap,\n                'num_predictions': len(pred_data), 'num_ground_truths': len(gt_boxes)\n            }\n            \n            class_aps.append(ap)\n            total_tp += tp\n            total_fp += fp\n            total_fn += fn\n        \n        # Overall metrics\n        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n        mAP = np.mean(class_aps)\n        \n        results[f'iou_{iou_thresh}'] = {\n            'overall': {\n                'mAP': mAP,\n                'precision': overall_precision,\n                'recall': overall_recall,\n                'f1': overall_f1,\n                'total_tp': total_tp,\n                'total_fp': total_fp,\n                'total_fn': total_fn\n            },\n            'per_class': class_results\n        }\n    \n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:38.022763Z","iopub.execute_input":"2025-05-27T09:05:38.023389Z","iopub.status.idle":"2025-05-27T09:05:38.038969Z","shell.execute_reply.started":"2025-05-27T09:05:38.023367Z","shell.execute_reply":"2025-05-27T09:05:38.038414Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def print_detailed_results(results):\n    \"\"\"Print detailed evaluation results\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ðŸ“ˆ COMPREHENSIVE OBJECT DETECTION EVALUATION RESULTS\")\n    print(\"=\"*80)\n    \n    for iou_key, metrics in results.items():\n        iou_thresh = iou_key.split('_')[1]\n        print(f\"\\nðŸŽ¯ IoU Threshold: {iou_thresh}\")\n        print(\"-\" * 50)\n        \n        overall = metrics['overall']\n        print(f\"ðŸ“Š Overall Metrics:\")\n        print(f\"   mAP:       {overall['mAP']:.4f}\")\n        print(f\"   Precision: {overall['precision']:.4f}\")\n        print(f\"   Recall:    {overall['recall']:.4f}\")\n        print(f\"   F1-Score:  {overall['f1']:.4f}\")\n        print(f\"   Total TP:  {overall['total_tp']}\")\n        print(f\"   Total FP:  {overall['total_fp']}\")\n        print(f\"   Total FN:  {overall['total_fn']}\")\n        \n        print(f\"\\nðŸ“‹ Per-Class Results:\")\n        print(f\"{'Class':<15} {'AP':<8} {'Prec':<8} {'Rec':<8} {'F1':<8} {'TP':<5} {'FP':<5} {'FN':<5}\")\n        print(\"-\" * 70)\n        \n        for class_name, class_metrics in metrics['per_class'].items():\n            print(f\"{class_name:<15} \"\n                  f\"{class_metrics['ap']:<8.4f} \"\n                  f\"{class_metrics['precision']:<8.4f} \"\n                  f\"{class_metrics['recall']:<8.4f} \"\n                  f\"{class_metrics['f1']:<8.4f} \"\n                  f\"{class_metrics['tp']:<5} \"\n                  f\"{class_metrics['fp']:<5} \"\n                  f\"{class_metrics['fn']:<5}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:45.461026Z","iopub.execute_input":"2025-05-27T09:05:45.461660Z","iopub.status.idle":"2025-05-27T09:05:45.467432Z","shell.execute_reply.started":"2025-05-27T09:05:45.461639Z","shell.execute_reply":"2025-05-27T09:05:45.466757Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\n\n\n\n\n\ndef create_evaluation_plots(results, save_dir=\"evaluation_plots\"):\n    \"\"\"Create visualization plots for the evaluation results\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Extract data for plotting\n    iou_50_results = results['iou_0.5']['per_class']\n    iou_75_results = results['iou_0.75']['per_class']\n    \n    # Prepare data\n    classes = list(iou_50_results.keys())\n    ap_50 = [iou_50_results[cls]['ap'] for cls in classes]\n    ap_75 = [iou_75_results[cls]['ap'] for cls in classes]\n    precision_50 = [iou_50_results[cls]['precision'] for cls in classes]\n    recall_50 = [iou_50_results[cls]['recall'] for cls in classes]\n    f1_50 = [iou_50_results[cls]['f1'] for cls in classes]\n    \n    # 1. AP comparison plot\n    plt.figure(figsize=(15, 8))\n    x = np.arange(len(classes))\n    width = 0.35\n    \n    plt.bar(x - width/2, ap_50, width, label='AP@0.5', alpha=0.8)\n    plt.bar(x + width/2, ap_75, width, label='AP@0.75', alpha=0.8)\n    \n    plt.xlabel('Classes')\n    plt.ylabel('Average Precision')\n    plt.title('Average Precision per Class (IoU 0.5 vs 0.75)')\n    plt.xticks(x, classes, rotation=45, ha='right')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/ap_comparison.png\", dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 2. Precision-Recall-F1 plot\n    plt.figure(figsize=(15, 8))\n    x = np.arange(len(classes))\n    width = 0.25\n    \n    plt.bar(x - width, precision_50, width, label='Precision', alpha=0.8)\n    plt.bar(x, recall_50, width, label='Recall', alpha=0.8)\n    plt.bar(x + width, f1_50, width, label='F1-Score', alpha=0.8)\n    \n    plt.xlabel('Classes')\n    plt.ylabel('Score')\n    plt.title('Precision, Recall, and F1-Score per Class (IoU@0.5)')\n    plt.xticks(x, classes, rotation=45, ha='right')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/precision_recall_f1.png\", dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 3. Summary metrics heatmap\n    metrics_data = []\n    for cls in classes:\n        metrics_data.append([\n            iou_50_results[cls]['ap'],\n            iou_50_results[cls]['precision'],\n            iou_50_results[cls]['recall'],\n            iou_50_results[cls]['f1']\n        ])\n    \n    plt.figure(figsize=(8, 12))\n    sns.heatmap(metrics_data, \n                xticklabels=['AP@0.5', 'Precision', 'Recall', 'F1'],\n                yticklabels=classes,\n                annot=True, fmt='.3f', cmap='Blues')\n    plt.title('Detection Metrics Heatmap')\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/metrics_heatmap.png\", dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"ðŸ“Š Plots saved to {save_dir}/\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:48.801170Z","iopub.execute_input":"2025-05-27T09:05:48.801434Z","iopub.status.idle":"2025-05-27T09:05:48.811632Z","shell.execute_reply.started":"2025-05-27T09:05:48.801414Z","shell.execute_reply":"2025-05-27T09:05:48.810852Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def save_results_to_csv(results, filename=\"detection_results.csv\"):\n    \"\"\"Save results to CSV file\"\"\"\n    data = []\n    \n    for iou_key, metrics in results.items():\n        iou_thresh = iou_key.split('_')[1]\n        \n        # Add overall metrics\n        overall = metrics['overall']\n        data.append({\n            'IoU_Threshold': iou_thresh,\n            'Class': 'Overall',\n            'mAP': overall['mAP'],\n            'AP': overall['mAP'],\n            'Precision': overall['precision'],\n            'Recall': overall['recall'],\n            'F1_Score': overall['f1'],\n            'TP': overall['total_tp'],\n            'FP': overall['total_fp'],\n            'FN': overall['total_fn']\n        })\n        \n        # Add per-class metrics\n        for class_name, class_metrics in metrics['per_class'].items():\n            data.append({\n                'IoU_Threshold': iou_thresh,\n                'Class': class_name,\n                'mAP': '',\n                'AP': class_metrics['ap'],\n                'Precision': class_metrics['precision'],\n                'Recall': class_metrics['recall'],\n                'F1_Score': class_metrics['f1'],\n                'TP': class_metrics['tp'],\n                'FP': class_metrics['fp'],\n                'FN': class_metrics['fn']\n            })\n    \n    df = pd.DataFrame(data)\n    df.to_csv(filename, index=False)\n    print(f\"ðŸ’¾ Results saved to {filename}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:05:53.344119Z","iopub.execute_input":"2025-05-27T09:05:53.344808Z","iopub.status.idle":"2025-05-27T09:05:53.350600Z","shell.execute_reply.started":"2025-05-27T09:05:53.344785Z","shell.execute_reply":"2025-05-27T09:05:53.349795Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def main():\n    # Configuration\n    MODEL_PATH = \"/kaggle/input/resnetyoloobject-detection/pytorch/default/3/best_model_new.pth\"  # Path to your trained model\n    DATA_PATH = \"/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007\"  # Path to VOC dataset\n    CONF_THRESHOLD = 0.3\n    IoU_THRESHOLDS = [0.5, 0.75, 0.9]\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"ðŸ”§ Using device: {device}\")\n    \n    # Load model\n    print(\"ðŸ“¥ Loading trained model...\")\n    model = load_model(MODEL_PATH, device)\n    print(\"âœ… Model loaded successfully!\")\n    \n    # Setup dataset and dataloader\n    transform = A.Compose([\n        A.Resize(448, 448),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n    \n    dataset = VOCDataset(DATA_PATH, transforms=transform)\n    dataloader = DataLoader(dataset, batch_size=8, shuffle=False, \n                           collate_fn=lambda x: tuple(zip(*x)))\n    \n    print(f\"ðŸ“Š Dataset loaded: {len(dataset)} images\")\n    \n    # Run evaluation\n    results = evaluate_detection_metrics(\n        model, dataloader, device, \n        iou_thresholds=IoU_THRESHOLDS,\n        conf_threshold=CONF_THRESHOLD\n    )\n    \n    # Print results\n    print_detailed_results(results)\n    \n    # Create plots\n    create_evaluation_plots(results)\n    \n    # Save to CSV\n    save_results_to_csv(results)\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"ðŸŽ¯ SUMMARY\")\n    print(\"=\"*80)\n    for iou_key, metrics in results.items():\n        iou_thresh = iou_key.split('_')[1]\n        overall = metrics['overall']\n        print(f\"IoU@{iou_thresh}: mAP={overall['mAP']:.4f}, \"\n              f\"Precision={overall['precision']:.4f}, \"\n              f\"Recall={overall['recall']:.4f}, \"\n              f\"F1={overall['f1']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:09:31.372783Z","iopub.execute_input":"2025-05-27T09:09:31.373347Z","iopub.status.idle":"2025-05-27T09:13:49.424112Z","shell.execute_reply.started":"2025-05-27T09:09:31.373327Z","shell.execute_reply":"2025-05-27T09:13:49.423452Z"}},"outputs":[{"name":"stdout","text":"ðŸ”§ Using device: cuda\nðŸ“¥ Loading trained model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model loaded successfully!\nðŸ“Š Dataset loaded: 4952 images\nðŸ”„ Running evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 619/619 [03:45<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“Š Calculating metrics for IoU threshold: 0.5\n\nðŸ“Š Calculating metrics for IoU threshold: 0.75\n\nðŸ“Š Calculating metrics for IoU threshold: 0.9\n\n================================================================================\nðŸ“ˆ COMPREHENSIVE OBJECT DETECTION EVALUATION RESULTS\n================================================================================\n\nðŸŽ¯ IoU Threshold: 0.5\n--------------------------------------------------\nðŸ“Š Overall Metrics:\n   mAP:       0.7629\n   Precision: 0.6248\n   Recall:    0.3212\n   F1-Score:  0.4243\n   Total TP:  4810\n   Total FP:  2889\n   Total FN:  10166\n\nðŸ“‹ Per-Class Results:\nClass           AP       Prec     Rec      F1       TP    FP    FN   \n----------------------------------------------------------------------\naeroplane       0.8253   0.6706   0.3666   0.4740   114   56    197  \nbicycle         0.9110   0.5860   0.3239   0.4172   126   89    263  \nbird            0.6836   0.5833   0.2795   0.3779   161   115   415  \nboat            0.4587   0.3960   0.1501   0.2177   59    90    334  \nbottle          0.5536   0.3588   0.0715   0.1193   47    84    610  \nbus             0.9251   0.7890   0.3386   0.4738   86    23    168  \ncar             0.8149   0.7056   0.4543   0.5527   700   292   841  \ncat             0.8697   0.7371   0.5000   0.5958   185   66    185  \nchair           0.6459   0.4822   0.1477   0.2262   203   218   1171 \ncow             0.7209   0.5265   0.4225   0.4688   139   125   190  \ndiningtable     0.8945   0.7632   0.0970   0.1721   29    9     270  \ndog             0.8724   0.7746   0.3566   0.4884   189   55    341  \nhorse           0.9151   0.8147   0.4785   0.6029   189   43    206  \nmotorbike       0.8863   0.7099   0.3117   0.4331   115   47    254  \nperson          0.6868   0.6144   0.3780   0.4681   1976  1240  3251 \npottedplant     0.5812   0.4595   0.0861   0.1451   51    60    541  \nsheep           0.6256   0.4444   0.2958   0.3552   92    115   219  \nsofa            0.7173   0.5906   0.2222   0.3229   88    61    308  \ntrain           0.8343   0.7010   0.4503   0.5484   136   58    166  \ntvmonitor       0.8359   0.7440   0.3463   0.4726   125   43    236  \n\nðŸŽ¯ IoU Threshold: 0.75\n--------------------------------------------------\nðŸ“Š Overall Metrics:\n   mAP:       0.3530\n   Precision: 0.2406\n   Recall:    0.1237\n   F1-Score:  0.1634\n   Total TP:  1852\n   Total FP:  5847\n   Total FN:  13124\n\nðŸ“‹ Per-Class Results:\nClass           AP       Prec     Rec      F1       TP    FP    FN   \n----------------------------------------------------------------------\naeroplane       0.3456   0.2471   0.1350   0.1746   42    128   269  \nbicycle         0.4506   0.2558   0.1414   0.1821   55    160   334  \nbird            0.3080   0.1812   0.0868   0.1174   50    226   526  \nboat            0.1280   0.0872   0.0331   0.0480   13    136   380  \nbottle          0.0703   0.0382   0.0076   0.0127   5     126   652  \nbus             0.6545   0.3853   0.1654   0.2314   42    67    212  \ncar             0.5382   0.3841   0.2472   0.3008   381   611   1160 \ncat             0.5438   0.3745   0.2541   0.3027   94    157   276  \nchair           0.2188   0.1116   0.0342   0.0524   47    374   1327 \ncow             0.2488   0.1591   0.1277   0.1417   42    222   287  \ndiningtable     0.2317   0.2632   0.0334   0.0593   10    28    289  \ndog             0.4605   0.3566   0.1642   0.2248   87    157   443  \nhorse           0.4633   0.3922   0.2304   0.2903   91    141   304  \nmotorbike       0.4414   0.2469   0.1084   0.1507   40    122   329  \nperson          0.2556   0.2037   0.1253   0.1552   655   2561  4572 \npottedplant     0.1961   0.0631   0.0118   0.0199   7     104   585  \nsheep           0.2121   0.1643   0.1093   0.1313   34    173   277  \nsofa            0.3591   0.2349   0.0884   0.1284   35    114   361  \ntrain           0.4297   0.3402   0.2185   0.2661   66    128   236  \ntvmonitor       0.5039   0.3333   0.1551   0.2117   56    112   305  \n\nðŸŽ¯ IoU Threshold: 0.9\n--------------------------------------------------\nðŸ“Š Overall Metrics:\n   mAP:       0.0592\n   Precision: 0.0257\n   Recall:    0.0132\n   F1-Score:  0.0175\n   Total TP:  198\n   Total FP:  7501\n   Total FN:  14778\n\nðŸ“‹ Per-Class Results:\nClass           AP       Prec     Rec      F1       TP    FP    FN   \n----------------------------------------------------------------------\naeroplane       0.0833   0.0235   0.0129   0.0166   4     166   307  \nbicycle         0.0902   0.0233   0.0129   0.0166   5     210   384  \nbird            0.0169   0.0109   0.0052   0.0070   3     273   573  \nboat            0.0101   0.0067   0.0025   0.0037   1     148   392  \nbottle          0.0000   0.0000   0.0000   0.0000   0     131   657  \nbus             0.3411   0.0917   0.0394   0.0551   10    99    244  \ncar             0.0823   0.0565   0.0363   0.0442   56    936   1485 \ncat             0.1207   0.0518   0.0351   0.0419   13    238   357  \nchair           0.0197   0.0048   0.0015   0.0022   2     419   1372 \ncow             0.0795   0.0152   0.0122   0.0135   4     260   325  \ndiningtable     0.0769   0.0263   0.0033   0.0059   1     37    298  \ndog             0.0235   0.0205   0.0094   0.0129   5     239   525  \nhorse           0.0556   0.0388   0.0228   0.0287   9     223   386  \nmotorbike       0.0113   0.0123   0.0054   0.0075   2     160   367  \nperson          0.0335   0.0208   0.0128   0.0159   67    3149  5160 \npottedplant     0.0000   0.0000   0.0000   0.0000   0     111   592  \nsheep           0.0000   0.0000   0.0000   0.0000   0     207   311  \nsofa            0.0363   0.0201   0.0076   0.0110   3     146   393  \ntrain           0.0676   0.0464   0.0298   0.0363   9     185   293  \ntvmonitor       0.0359   0.0238   0.0111   0.0151   4     164   357  \nðŸ“Š Plots saved to evaluation_plots/\nðŸ’¾ Results saved to detection_results.csv\n\n================================================================================\nðŸŽ¯ SUMMARY\n================================================================================\nIoU@0.5: mAP=0.7629, Precision=0.6248, Recall=0.3212, F1=0.4243\nIoU@0.75: mAP=0.3530, Precision=0.2406, Recall=0.1237, F1=0.1634\nIoU@0.9: mAP=0.0592, Precision=0.0257, Recall=0.0132, F1=0.0175\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}